{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HetioNet based Metapath Extraction\n",
    "\n",
    "In this notebook we will showcase the efficiency of PAM on Metapath Extraction.\n",
    "\n",
    "We will use the [HetioNet](https://github.com/hetio/hetionet) KG for this task.\n",
    "\n",
    "The process is simple:\n",
    "1. First download the dataset. The .tsv with the triples is expected to be in folder in the same directory as this notebook with the name \"data\".\n",
    "2. Create the PAMs P and P^2 from the collection of triples.\n",
    "3. Find metapaths that may imply the treats relation based on a simple check: $P[C,D] = ``Compound$  $treats$ $Disease\"$ and $P^2[C,D] = v \\neq 0$.\n",
    "\n",
    "According to 3, the metapath $m_1$ that we will extract in that case is will be: $m_1: v \\implies \"treats\"$.\n",
    "\n",
    "Finally, after aggregating all Metapaths, we will check for scenarios were the metapath can maybe provide us with new possible treatmens with an explanation in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scipy\n",
    "import tqdm\n",
    "import time\n",
    "import sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import permutations, product\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    get_prime_map_from_rel,\n",
    "    get_primefactors,\n",
    "    load_data,\n",
    "    get_sparsity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "#   torch.manual_seed(seed)\n",
    "#   torch.cuda.manual_seed(seed)\n",
    "#   torch.backends.cudnn.deterministic = True\n",
    "#   os.environ('PYTHONHASHSEED') = str(seed)\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We load the data. Because in the original work they allow path traverals that do not follow the original direction, we also add the inverse edges between nodes to allow for such paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original # of unique rels: ['GpBP' 'GiG' 'CrC' 'DdG' 'DpS' 'DlA' 'CtD' 'CbG' 'CuG' 'DrD' 'DaG' 'CpD'\n",
      " 'AdG' 'AuG' 'GcG' 'GpMF' 'PCiC' 'GpCC' 'Gr>G' 'CdG' 'DuG' 'GpPW' 'CcSE'\n",
      " 'AeG']\n",
      "Will add the inverse train edges as well..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3606630/646125673.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_train = df_train.append(df_train_inv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deduplication: 4500394\n",
      "After deduplication: 4500394\n",
      "# of unique rels: 44 \t | # of unique nodes: 45158\n"
     ]
    }
   ],
   "source": [
    "time_s = time.time()\n",
    "add_inverse_edges = \"YES\"\n",
    "df_train = pd.read_csv(\"./data/Hetionet/hetionet-v1.0-edges.tsv\", sep='\\t')\n",
    "df_train.columns = ['head', 'rel', 'tail']\n",
    "df_train.dropna(inplace=True)\n",
    "print(f'Original # of unique rels: {df_train[\"rel\"].unique()}')\n",
    "\n",
    "if \"YES\" in add_inverse_edges:\n",
    "    print(f\"Will add the inverse train edges as well..\")\n",
    "    df_train[\"rel\"] = df_train[\"rel\"].astype(str)\n",
    "    df_train_inv = df_train.copy()\n",
    "    df_train[\"rel\"] = df_train[\"rel\"].apply(lambda x: x[::-1])\n",
    "    df_train_inv[\"head\"] = df_train[\"tail\"]\n",
    "    df_train_inv[\"tail\"] = df_train[\"head\"]\n",
    "    if add_inverse_edges == \"YES__INV\":\n",
    "        df_train_inv[\"rel\"] = df_train[\"rel\"] + \"__INV\"\n",
    "    df_train = df_train.append(df_train_inv)\n",
    "    print(f'Before deduplication: {df_train.shape[0]}')\n",
    "    df_train.drop_duplicates(inplace=True)  \n",
    "    print(f'After deduplication: {df_train.shape[0]}')\n",
    "\n",
    "unique_rels = sorted(list(df_train['rel'].unique()))\n",
    "unique_nodes = sorted(set(df_train['head'].values.tolist() + df_train['tail'].values.tolist()))\n",
    "print(f'# of unique rels: {len(unique_rels)} \\t | # of unique nodes: {len(unique_nodes)}')\n",
    "\n",
    "\n",
    "time_prev = time.time()\n",
    "time_needed = time_prev - time_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the relations and nodes\n",
    "\n",
    "The relationsa are mapepd to primes according to PAM paradigm and nodes to integers for the rows, columns of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.01345 secs (0.00 mins)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_s = time.time()\n",
    "\n",
    "node2id = {}\n",
    "id2node = {}\n",
    "for i, node in enumerate(unique_nodes):\n",
    "    node2id[node] = i\n",
    "    id2node[i] = node\n",
    "\n",
    "\n",
    "\n",
    "rel2id, id2rel = get_prime_map_from_rel(unique_rels, \n",
    "                                        starting_value=2, \n",
    "                                        spacing_strategy=\"step_100\")\n",
    "\n",
    "\n",
    "time_prev = time.time()\n",
    "time_needed = time_prev - time_s\n",
    "print(f'Total time: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdG': 3, 'AeG': 107, 'AlD': 211, 'AuG': 313, 'CCpG': 419, 'CbG': 521, 'CcSE': 631, 'CdG': 733, 'CiCP': 839, 'CpD': 941, 'CrC': 1049, 'CtD': 1151, 'CuG': 1259, 'DaG': 1361, 'DdG': 1471, 'DlA': 1579, 'DpC': 1693, 'DpS': 1801, 'DrD': 1907, 'DtC': 2011, 'DuG': 2113, 'EScC': 2221}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AdG': 3,\n",
       " 'AeG': 107,\n",
       " 'AlD': 211,\n",
       " 'AuG': 313,\n",
       " 'CCpG': 419,\n",
       " 'CbG': 521,\n",
       " 'CcSE': 631,\n",
       " 'CdG': 733,\n",
       " 'CiCP': 839,\n",
       " 'CpD': 941,\n",
       " 'CrC': 1049,\n",
       " 'CtD': 1151,\n",
       " 'CuG': 1259,\n",
       " 'DaG': 1361,\n",
       " 'DdG': 1471,\n",
       " 'DlA': 1579,\n",
       " 'DpC': 1693,\n",
       " 'DpS': 1801,\n",
       " 'DrD': 1907,\n",
       " 'DtC': 2011,\n",
       " 'DuG': 2113,\n",
       " 'EScC': 2221,\n",
       " 'FMpG': 2333,\n",
       " 'G>rG': 2437,\n",
       " 'GaD': 2539,\n",
       " 'GbC': 2647,\n",
       " 'GcG': 2749,\n",
       " 'GdA': 2851,\n",
       " 'GdC': 2953,\n",
       " 'GdD': 3061,\n",
       " 'GeA': 3163,\n",
       " 'GiG': 3271,\n",
       " 'GpBP': 3373,\n",
       " 'GpCC': 3491,\n",
       " 'GpMF': 3593,\n",
       " 'GpPW': 3697,\n",
       " 'Gr>G': 3803,\n",
       " 'GuA': 3907,\n",
       " 'GuC': 4013,\n",
       " 'GuD': 4127,\n",
       " 'PBpG': 4229,\n",
       " 'PCiC': 4337,\n",
       " 'SpD': 4441,\n",
       " 'WPpG': 4547}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_rel2id = {}\n",
    "for i, (key, value) in enumerate(rel2id.items()):\n",
    "    if i == len(rel2id) / 2:\n",
    "        break\n",
    "    else:\n",
    "        original_rel2id[key] = value\n",
    "print(original_rel2id)\n",
    "if add_inverse_edges == 'YES__INV':\n",
    "    assert len(original_rel2id) * 2 == len(rel2id)\n",
    "rel2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the original PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_s = time.time()\n",
    "\n",
    "val_rels_dict_product = defaultdict(lambda:1)\n",
    "for i, row in df_train.iterrows():\n",
    "    val_rels_dict_product[(node2id[row['head']], node2id[row['tail']])] *= rel2id[str(row['rel'])]\n",
    "    \n",
    "row = []\n",
    "col = []\n",
    "val_rels_prod = []\n",
    "for key, val in val_rels_dict_product.items():\n",
    "    row.append(key[0])\n",
    "    col.append(key[1])\n",
    "    val_rels_prod.append(val)\n",
    "    \n",
    "\n",
    "    \n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "val_rels_prod = np.array(val_rels_prod)\n",
    "\n",
    "print('Will create the sparse matrices')\n",
    "\n",
    "\n",
    "A_big = csr_matrix((val_rels_prod, (row, col)), shape=(len(unique_nodes), len(unique_nodes)))\n",
    "\n",
    "\n",
    "print('Created the sparse matrices')\n",
    "sparsity = get_sparsity(A_big)\n",
    "print(A_big.shape, f\"Sparsity: {sparsity:.2f} %\")\n",
    "\n",
    "time_prev = time.time()\n",
    "time_needed = time_prev - time_s\n",
    "print(f'Total time: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the 2-hop PAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity 2-hop: 77.54 %\n",
      "2: 25.84444 secs (0.43 mins)\n",
      "Total time: 25.84455 secs (0.43 mins)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_order = 2\n",
    "\n",
    "\n",
    "power_A = [A_big]\n",
    "\n",
    "time_s = time.time()\n",
    "time_prev = time_s\n",
    "for ii in range(1, max_order):\n",
    "    updated_power = (power_A[-1] * A_big)\n",
    "    updated_power.sort_indices()\n",
    "    updated_power.eliminate_zeros()\n",
    "    power_A.append(updated_power)\n",
    "    print(f\"Sparsity {ii + 1}-hop: {100 * (1 - updated_power.nnz/(updated_power.shape[0]**2)):.2f} %\")\n",
    "    time_prev = time.time()\n",
    "    time_needed = time_prev - time_s\n",
    "    print(f'{ii + 1}: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')\n",
    "time_prev = time.time()\n",
    "time_needed = time_prev - time_s\n",
    "print(f'Total time: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')\n",
    "len(power_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform this to a sparse 3d tensor (for easiness in extracting the rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity 1-hop: 99.79 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:23, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity 2-hop: 77.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 25.86248 secs (0.43 mins)\n"
     ]
    }
   ],
   "source": [
    "def to_sparse_3d(tensorlist):\n",
    "    for r_i, sparse_ in tqdm.tqdm(enumerate(tensorlist)):\n",
    "        rows_i, cols_i = sparse_.nonzero()\n",
    "        vals_i = np.squeeze(np.asarray(sparse_[rows_i, cols_i]))\n",
    "        if r_i == 0:\n",
    "            rows = rows_i\n",
    "            cols = cols_i\n",
    "            rs = r_i*np.ones((len(rows_i),))\n",
    "            vals = vals_i\n",
    "        else:\n",
    "            rows = np.hstack((rows, rows_i))\n",
    "            cols = np.hstack((cols, cols_i))\n",
    "            rs = np.hstack((rs, r_i*np.ones((len(rows_i),))))\n",
    "            vals = np.hstack((vals, vals_i))\n",
    "        print(f\"Sparsity {r_i+1}-hop: {100 * (1 - sparse_.nnz/(sparse_.shape[0]**2)):.2f} %\")\n",
    "    r,c = sparse_.shape\n",
    "    \n",
    "    coo_tensor = sparse.COO(np.array([rows,cols,rs.astype(int)]), vals, shape=(r,c, len(tensorlist)))\n",
    "    return coo_tensor\n",
    "time_prev = time.time()\n",
    "A_tensor = to_sparse_3d(power_A[:4])\n",
    "time_needed = time_prev - time_s\n",
    "print(f'Total time: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract possible metapaths\n",
    "\n",
    "Now we are going to extract metapaths (or also called rules, because the imply the treats relation) that express the ``*Compound treats Disease*\" relation.\n",
    "\n",
    "1. We first find all pairs of $(i,j)$ for which $P[i,j] = 1151$, where $1151$ is the prime that ``*Compound treats Disease*\" is mapped to.\n",
    "2. Then we keep track of all the different values for these $(i,j)$,that $P^2[i,j] = v \\neq 0$.\n",
    "3. We aggregate all of these metapaths in a DataFrame, keeping also frequencies and other characteristics of the metapath (e.g. support of the rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-hop level rules: 479\n",
      "Total # rules: 479\n",
      "Total time: 132.46191 secs (2.21 mins)\n",
      "Conf >= 0.01 rules: 459\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_rel</th>\n",
       "      <th>body</th>\n",
       "      <th>head_body_count</th>\n",
       "      <th>body_count</th>\n",
       "      <th>head_count</th>\n",
       "      <th>hop</th>\n",
       "      <th>type</th>\n",
       "      <th>conf</th>\n",
       "      <th>head_coverage</th>\n",
       "      <th>lift</th>\n",
       "      <th>conviction</th>\n",
       "      <th>body_head_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1151</td>\n",
       "      <td>2194957</td>\n",
       "      <td>23</td>\n",
       "      <td>1284</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.033189</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-7.046146e+02</td>\n",
       "      <td>2194957_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1151</td>\n",
       "      <td>1207399</td>\n",
       "      <td>20</td>\n",
       "      <td>711</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-7.120216e+02</td>\n",
       "      <td>1207399_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1151</td>\n",
       "      <td>1322819</td>\n",
       "      <td>18</td>\n",
       "      <td>11478</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-6.930800e+02</td>\n",
       "      <td>1322819_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1151</td>\n",
       "      <td>2645638</td>\n",
       "      <td>17</td>\n",
       "      <td>2795</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.006082</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-6.962277e+02</td>\n",
       "      <td>2645638_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1151</td>\n",
       "      <td>4389914</td>\n",
       "      <td>11</td>\n",
       "      <td>242</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-7.249448e+02</td>\n",
       "      <td>4389914_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1151</td>\n",
       "      <td>13356270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-6.920000e+07</td>\n",
       "      <td>13356270_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1151</td>\n",
       "      <td>13286587</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>-1.383972e+03</td>\n",
       "      <td>13286587_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1151</td>\n",
       "      <td>13189358</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-6.920000e+07</td>\n",
       "      <td>13189358_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1151</td>\n",
       "      <td>13176968</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-6.920000e+07</td>\n",
       "      <td>13176968_1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1151</td>\n",
       "      <td>8020920660232</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>693</td>\n",
       "      <td>2</td>\n",
       "      <td>same_cell</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-6.920000e+07</td>\n",
       "      <td>8020920660232_1151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     head_rel           body  head_body_count  body_count  head_count  hop  \\\n",
       "0        1151        2194957               23        1284         693    2   \n",
       "1        1151        1207399               20         711         693    2   \n",
       "2        1151        1322819               18       11478         693    2   \n",
       "3        1151        2645638               17        2795         693    2   \n",
       "4        1151        4389914               11         242         693    2   \n",
       "..        ...            ...              ...         ...         ...  ...   \n",
       "200      1151       13356270                1           1         693    2   \n",
       "199      1151       13286587                1           2         693    2   \n",
       "198      1151       13189358                1           1         693    2   \n",
       "197      1151       13176968                1           1         693    2   \n",
       "478      1151  8020920660232                1           1         693    2   \n",
       "\n",
       "          type      conf  head_coverage      lift    conviction  \\\n",
       "0    same_cell  0.017913       0.033189  0.000026 -7.046146e+02   \n",
       "1    same_cell  0.028129       0.028860  0.000041 -7.120216e+02   \n",
       "2    same_cell  0.001568       0.025974  0.000002 -6.930800e+02   \n",
       "3    same_cell  0.006082       0.024531  0.000009 -6.962277e+02   \n",
       "4    same_cell  0.045455       0.015873  0.000066 -7.249448e+02   \n",
       "..         ...       ...            ...       ...           ...   \n",
       "200  same_cell  1.000000       0.001443  0.001443 -6.920000e+07   \n",
       "199  same_cell  0.500000       0.001443  0.000722 -1.383972e+03   \n",
       "198  same_cell  1.000000       0.001443  0.001443 -6.920000e+07   \n",
       "197  same_cell  1.000000       0.001443  0.001443 -6.920000e+07   \n",
       "478  same_cell  1.000000       0.001443  0.001443 -6.920000e+07   \n",
       "\n",
       "          body_head_str  \n",
       "0          2194957_1151  \n",
       "1          1207399_1151  \n",
       "2          1322819_1151  \n",
       "3          2645638_1151  \n",
       "4          4389914_1151  \n",
       "..                  ...  \n",
       "200       13356270_1151  \n",
       "199       13286587_1151  \n",
       "198       13189358_1151  \n",
       "197       13176968_1151  \n",
       "478  8020920660232_1151  \n",
       "\n",
       "[479 rows x 12 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rules = []\n",
    "\n",
    "max_hop = len(power_A)\n",
    "laplace_smoothing_alpha = 0\n",
    "\n",
    "time_s = time.time()\n",
    "\n",
    "# Focus on the pair of nodes that are directly connected with a relation\n",
    "# In order to find rules/metapaths that indicate such connections\n",
    "# Only interested in relations of type 1151 which is CtD == Compound treats Disease\n",
    "rows, cols = (A_big==1151).nonzero()\n",
    "head_rels = np.array(A_big[rows,cols]).flatten()\n",
    "\n",
    "for cur_hop in range(1,max_hop):\n",
    "    cur_power = power_A[cur_hop]\n",
    "    cur_rules = []\n",
    "    for index_nonzero, body_rel in enumerate(np.array(cur_power[rows, cols]).flatten()):\n",
    "        cur_prime_factors = get_primefactors(head_rels[index_nonzero])\n",
    "        # Same cell rules\n",
    "        if body_rel != 0:\n",
    "            for head_rel in cur_prime_factors:\n",
    "                # Only interested in real relations as head of a rule\n",
    "                if head_rel in [1151]:\n",
    "                    cur_rules.append(\n",
    "                        {\n",
    "                            'head_rel':head_rel, \n",
    "                            \"body\":body_rel, \n",
    "                            \"hop\":cur_hop + 1, \n",
    "                            'type':'same_cell'\n",
    "                        }\n",
    "                    )\n",
    "                \n",
    "            \n",
    "    df_rules = pd.DataFrame(cur_rules)\n",
    "    grouped_by_head_and_body = df_rules.groupby([\"head_rel\", 'type'])['body'].value_counts().to_dict()\n",
    "    head_support = df_rules['head_rel'].value_counts().to_dict()\n",
    "    # This is only using support from the cases we have the rule appear\n",
    "    #body_support = df_rules.value_counts('body').to_dict()\n",
    "    # This is for all occurrences of the body\n",
    "    body_support = pd.Series(cur_power.data).value_counts().to_dict()\n",
    "    for key in grouped_by_head_and_body:\n",
    "        head_value, type_, body_value  = key\n",
    "        all_rules.append({\n",
    "            \"head_rel\": head_value, \n",
    "            'body':body_value, \n",
    "            'head_body_count':grouped_by_head_and_body[key], \n",
    "            'body_count':body_support[body_value],\n",
    "            'head_count':head_support[head_value],\n",
    "            'hop':cur_hop + 1,\n",
    "            'type':type_})\n",
    "    print(f'{cur_hop + 1}-hop level rules: {len(grouped_by_head_and_body)}')\n",
    "print(f'Total # rules: {len(all_rules)}')\n",
    "\n",
    "time_prev = time.time()\n",
    "time_needed = time_prev - time_s\n",
    "print(f'Total time: {time_needed:.5f} secs ({time_needed/60:.2f} mins)')\n",
    "\n",
    "\n",
    "all_rules_df = pd.DataFrame(all_rules)\n",
    "all_rules_df['conf'] = all_rules_df['head_body_count'] / (all_rules_df['body_count'] + laplace_smoothing_alpha)\n",
    "all_rules_df['head_coverage'] = all_rules_df['head_body_count'] / (all_rules_df['head_count'])\n",
    "all_rules_df['lift'] = all_rules_df['head_body_count'] / (all_rules_df['head_count'] * all_rules_df['body_count'])\n",
    "all_rules_df['conviction'] = (1 - all_rules_df['head_count']) / (1 - all_rules_df['conf'] + 0.00001)\n",
    "\n",
    "print(f'Conf >= 0.01 rules: {len(all_rules_df[all_rules_df[\"conf\"] >=0.01])}')\n",
    "all_rules_df['body_head_str'] = all_rules_df['body'].astype(str) + \"_\" + all_rules_df['head_rel'].astype(str)\n",
    "all_rules_df.sort_values(['head_body_count'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for the wanted metapaths.\n",
    "\n",
    "We will first make sure we find the important metapaths as generated by the HetioNet authors.\n",
    "\n",
    "We first map these metapaths to the corresponding relational chains in the form of the product of the primes of the relations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HetioNet</th>\n",
       "      <th>Mapped</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CbGaD</td>\n",
       "      <td>[521, 2539]</td>\n",
       "      <td>1322819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CdGuD</td>\n",
       "      <td>[733, 4127]</td>\n",
       "      <td>3025091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrCtD</td>\n",
       "      <td>[1049, 1151]</td>\n",
       "      <td>1207399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CtDrD</td>\n",
       "      <td>[1151, 1907]</td>\n",
       "      <td>2194957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CuGdD</td>\n",
       "      <td>[1259, 3061]</td>\n",
       "      <td>3853799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HetioNet        Mapped     Prod\n",
       "0    CbGaD   [521, 2539]  1322819\n",
       "1    CdGuD   [733, 4127]  3025091\n",
       "2    CrCtD  [1049, 1151]  1207399\n",
       "3    CtDrD  [1151, 1907]  2194957\n",
       "4    CuGdD  [1259, 3061]  3853799"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metapaths = \"\"\"CbGaD,\n",
    "CdGuD,\n",
    "CrCtD,\n",
    "CtDrD,\n",
    "CuGdD\"\"\"\n",
    "metapaths = metapaths.replace(\"\\n\", \"\").split(\",\")\n",
    "meta_mapped = []\n",
    "step = 3\n",
    "for metapath in metapaths:\n",
    "    cur_index = 0\n",
    "    cur_list = []\n",
    "    while cur_index < len(metapath) - 1:\n",
    "        cur_name = metapath[cur_index:cur_index+step]\n",
    "        cur_list.append(rel2id[cur_name])\n",
    "        cur_index += step -1\n",
    "    meta_mapped.append({\n",
    "        \"HetioNet\": metapath,\n",
    "        'Mapped':cur_list,\n",
    "        'Prod': np.prod(cur_list)\n",
    "    })\n",
    "metapath_df = pd.DataFrame(meta_mapped)\n",
    "metapath_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting the Metapaths\n",
    "\n",
    "As explained in the paper, the PAM represenation allows for combinations of relations in the metapaths.\n",
    "\n",
    "To handle these we create the appropriate mappings/namings for the specific metapath examples.\n",
    "\n",
    "For example the Metapath *{CuG+CbG}GdD* expresses a relational path of the form:\n",
    "1. It starts from a compound C.\n",
    "2. One edge connects this compound through the relation *upregulates* to with a Gene G (*CuG*).\n",
    "3. Another edge connects this compound through the relation *binds* to with a Gene G (*CbG*).\n",
    "3. Finally, an edge going out from this Gene of the type \"downregulates\" connects it to disease D (*GdD*)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3606630/2063964847.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metapath_df = metapath_df.append(pd.DataFrame([\n"
     ]
    }
   ],
   "source": [
    "metapath_df = metapath_df.append(pd.DataFrame([\n",
    "    {\"HetioNet\":\"{CbG+CuG}GaD\", \"Mapped\":[1780,2539], 'Prod':4519420},\n",
    "    {\"HetioNet\":\"{CbG+GdC}GaD\", \"Mapped\":[3474,2539], 'Prod':8820486},\n",
    "    {\"HetioNet\":\"{CuG+CbG}GdD\", \"Mapped\":[1780,3061], 'Prod':5448580},\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod2hetio = dict(zip(metapath_df.Prod, metapath_df.HetioNet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = pd.read_csv(\"./data/Hetionet/hetionet-v1.0-nodes.tsv\", sep='\\t')\n",
    "df_nodes['node_id'] = df_nodes['id'].map(node2id)\n",
    "nodeid2name = dict(zip(df_nodes['node_id'], df_nodes['name']))\n",
    "name2nodeid = dict(zip(df_nodes['name'], df_nodes['node_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase the example of the paper\n",
    "\n",
    "In the following we explore the metapath *CrC+Ct/pD* (a compound relates to another compound, which both paliates and treats a Disease).\n",
    "\n",
    "Two pair of *Compounds*, *Diseases*  that **were not directly linked with a ``treats'' relationship in the KG** but are linked through the metapath are:\n",
    "\n",
    "\n",
    "1. *Hexoprenaline - CrC+Ct/pD -> asthma* : indicating that maybe Hexoprenaline should be used for asthma treatment\n",
    "2. *Fluticasone Propionate - CrC+Ct/pD -> hematologic cancer: : indicating that maybe Fluticasone Propionate should be used for hematologic cancer treatment\n",
    "\n",
    "These were validated according to the related bibliography.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desoximetasone - CrC+Ct/pD -> hematologic cancer\n",
      "Golden truth: Desoximetasone  not connected  hematologic cancer\n",
      "\n",
      "Fluticasone Propionate - CrC+Ct/pD -> hematologic cancer\n",
      "Golden truth: Fluticasone Propionate  not connected  hematologic cancer\n",
      "\n",
      "Orciprenaline - CrC+Ct/pD -> asthma\n",
      "Golden truth: Orciprenaline  - CtD-> asthma\n",
      "\n",
      "Methyldopa - CrC+Ct/pD -> asthma\n",
      "Golden truth: Methyldopa  not connected  asthma\n",
      "\n",
      "Exemestane - CrC+Ct/pD -> hematologic cancer\n",
      "Golden truth: Exemestane  not connected  hematologic cancer\n",
      "\n",
      "Dexamethasone - CrC+Ct/pD -> hematologic cancer\n",
      "Golden truth: Dexamethasone  - CtD-> hematologic cancer\n",
      "\n",
      "Droxidopa - CrC+Ct/pD -> asthma\n",
      "Golden truth: Droxidopa  not connected  asthma\n",
      "\n",
      "Hexoprenaline - CrC+Ct/pD -> asthma\n",
      "Golden truth: Hexoprenaline  not connected  asthma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows, cols = (power_A[1]==3401907).nonzero()\n",
    "for i, row_i in enumerate(rows):\n",
    "    col_i = cols[i]\n",
    "    print(f\"{nodeid2name[row_i]} - CrC+Ct/pD -> {nodeid2name[col_i]}\")\n",
    "    try:\n",
    "        gt_rel = \" - \" + id2rel[power_A[0][row_i, col_i]] + \"->\"\n",
    "    except KeyError:\n",
    "        gt_rel = \" not connected \"\n",
    "    print(f\"Golden truth: {nodeid2name[row_i]} {gt_rel} {nodeid2name[col_i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Metapaths\n",
    "\n",
    "Finally, here we present all that were found due to the PAMs extended language bias.\n",
    "\n",
    "We translate them to the corresponding metapaths on the KG.\n",
    "\n",
    "To extract them we need an ILP solver to decompose the aggregated product of primes to the corresponding base values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2194957 := 1.0 * 2194957 => CtDrD\n",
      "1 1207399 := 1.0 * 1207399 => CrCtD\n",
      "2 2414798 := 2.0 * 1207399 => CrCtD\n",
      "3 4389914 := 2.0 * 2194957 => CtDrD\n",
      "4 3622197 := 3.0 * 1207399 => CrCtD\n",
      "5 6584871 := 3.0 * 2194957 => CtDrD\n",
      "6 3402356 := 1.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "7 4829596 := 4.0 * 1207399 => CrCtD\n",
      "8 5597313 := 1.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "9 4609755 := 2.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "10 6036995 := 5.0 * 1207399 => CrCtD\n",
      "11 7244394 := 6.0 * 1207399 => CrCtD\n",
      "12 7792270 := 1.0 * CrCtD + 3.0 * CtDrD  => {CuG+CbG}GdD\n",
      "13 8779828 := 4.0 * 2194957 => CtDrD\n",
      "16 8451793 := 7.0 * 1207399 => CrCtD\n",
      "17 11634308 := 6.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "19 5817154 := 3.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "20 7024553 := 4.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "21 8231952 := 5.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "22 8999669 := 2.0 * CrCtD + 3.0 * CtDrD  => {CuG+CbG}GdD\n",
      "23 9219510 := 4.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "24 10426909 := 5.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "28 8012111 := 3.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "29 9439351 := 6.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "30 9659192 := 8.0 * 1207399 => CrCtD\n",
      "31 10207068 := 3.0 * CrCtD + 3.0 * CtDrD  => {CuG+CbG}GdD\n",
      "32 10974785 := 5.0 * 2194957 => CtDrD\n",
      "33 11194626 := 2.0 * CrCtD + 4.0 * CtDrD  => {CuG+CbG}GdD\n",
      "34 12621866 := 5.0 * CrCtD + 3.0 * CtDrD  => {CuG+CbG}GdD\n",
      "35 14049106 := 8.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "36 14268947 := 10.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "37 14377141 := 1.0 * CrCtD + 6.0 * CtDrD  => {CuG+CbG}GdD\n",
      "45 6804712 := 2.0 * CrCtD + 2.0 * CtDrD  => {CuG+CbG}GdD\n",
      "49 9987227 := 1.0 * CrCtD + 4.0 * CtDrD  => {CuG+CbG}GdD\n",
      "50 10646750 := 7.0 * CrCtD + 1.0 * CtDrD  => {CuG+CbG}GdD\n",
      "51 10866591 := 9.0 * 1207399 => CrCtD\n",
      "53 11414467 := 4.0 * CrCtD + 3.0 * CtDrD  => {CuG+CbG}GdD\n",
      "54 13169742 := 6.0 * 2194957 => CtDrD\n"
     ]
    }
   ],
   "source": [
    "from utils import ILP_solver\n",
    "metapaths_found = []\n",
    "for i, body_val in enumerate(all_rules_df['body']):\n",
    "    found_ = False\n",
    "    for target_score in metapath_df['Prod']:\n",
    "        if body_val % target_score == 0:\n",
    "            print(f'{i} {body_val} := {body_val/target_score} * {target_score} => {prod2hetio[target_score]}')\n",
    "            metapaths_found.append(prod2hetio[target_score])\n",
    "            found_ = True\n",
    "    if not(found_):\n",
    "        for num_path in range(1,50):\n",
    "            solved = ILP_solver(metapath_df['Prod'].values.tolist(), body_val, num_path)\n",
    "            if solved:\n",
    "                str_map = \"\"\n",
    "                for key, iter_ in solved.items():\n",
    "                    if iter_ > 0:\n",
    "                        str_map += f\"{iter_} * {prod2hetio[int(key)]} + \"\n",
    "                str_map = str_map[:-2]\n",
    "                print(f'{i} {body_val} := {str_map} => {prod2hetio[target_score]}')\n",
    "                metapaths_found.append(prod2hetio[target_score])\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first two paths are *CtDrD* and *CrCtD*, which are mapped to the products $2194957$ and $1207399$ correspondingly.\n",
    "\n",
    "The third (aggregated) path found is of value $2414798$ which is $1207399\\times2$, which essentially expresses 2 different $1207399$ paths, that is 2 distinct *CrCtD* paths.\n",
    "\n",
    "And so on and so forth."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c01865829f576eaff008cd308e2ea27c464a4f8737e86651bac34bc9029f3de9"
  },
  "kernelspec": {
   "display_name": "prime",
   "language": "python",
   "name": "prime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
